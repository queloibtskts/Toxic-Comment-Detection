{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7fa77e863e90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "#pip install nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import en_core_web_sm\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/xiyanzhang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'data/train.csv')\n",
    "X_train = train.comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804874,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieved from https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n",
    "\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus):\n",
    "    '''This pre-processing function lemmatizes words, makes a commment case-insensitive, removes punctuation,\n",
    "    removes stop words and removes single characters. '''\n",
    "    \n",
    "    # convert df to list; *corpus* is of a form of a list of str\n",
    "    original_indices = corpus.index\n",
    "    corpus = corpus.to_list()\n",
    "    \n",
    "    # initialize lemmatizer and a list of stopwords\n",
    "    lemmatizer = spacy.load('en_core_web_sm', disable = ['parser', 'ner'])\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        text = corpus[i] # one comment\n",
    "        \n",
    "        # convert text to lower case\n",
    "        text = text.lower()\n",
    "        \n",
    "        # remove emoji\n",
    "        text = deEmojify(text)\n",
    "        \n",
    "        # remove url links\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        text = re.sub(r\"www\\S+\", \"\", text)\n",
    "        \n",
    "        # remove punctuations and numbers but save the \"'\"\n",
    "        symbols = \"—…,!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~0123456789\"\n",
    "        for sym in symbols:\n",
    "            text = np.char.replace(text, sym, ' ') # replace symbols by one space\n",
    "\n",
    "        # tokenize text\n",
    "        text_tokens = word_tokenize(str(text))\n",
    "\n",
    "        # spaCy Lemmatization\n",
    "        doc = lemmatizer(\" \".join([tk for tk in text_tokens]))\n",
    "        #print(doc)\n",
    "        corpus[i] = \" \".join([token.lemma_ for token in doc]) # Extract the lemma for each token and join\n",
    "    return pd.Series(corpus, index=original_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
