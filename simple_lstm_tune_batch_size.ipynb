{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate, Dropout\n",
    "from keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, LSTM\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Title: Scikit-learn: How to obtain True Positive, True Negative, False Positive and False Negative\n",
    "Author: invoketheshell, & Rasoul\n",
    "Date: 2015\n",
    "Availability: https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal \"\"\"\n",
    "def perf_measure(y_actual, y_hat):\n",
    "    y_actual = y_actual.to_list()\n",
    "    y_hat = y_hat.to_list()\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1: TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]: FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0: TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]: FN += 1\n",
    "    return(TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILES = [\n",
    "    'crawl-300d-2M.gensim',\n",
    "    'glove.840B.300d.gensim'\n",
    "]\n",
    "\n",
    "# tuning batch size\n",
    "BATCH_SIZE = 512\n",
    "# BATCH_SIZE = 5000\n",
    "\n",
    "LSTM_UNITS = 128 # output vector dimension of each lstm cell\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "EPOCHS = 20\n",
    "DROPOUT_RATE = 0.5\n",
    "TRAIN_PERCENT = 0.8\n",
    "VALID_PERCENT = 0.25 # 25% from the training set is equiv to 20% from the whole dataset\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "TARGET_COLUMN = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Title: Simple LSTM\n",
    "Author: thousandvoices\n",
    "Date: 2019\n",
    "Code version: 8\n",
    "Availability: https://www.kaggle.com/thousandvoices/simple-lstm?scriptVersionId=16109977 \"\"\"\n",
    "def build_matrix(word_index, path):\n",
    "    unknown_words = []\n",
    "    embedding_index = KeyedVectors.load(path, mmap='r')\n",
    "    embedding_matrix = np.zeros((MAX_FEATURES + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        if i <= MAX_FEATURES:\n",
    "            try:\n",
    "                embedding_matrix[i] = embedding_index[word]\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    embedding_matrix[i] = embedding_index[word.lower()]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        embedding_matrix[i] = embedding_index[word.title()]\n",
    "                    except KeyError:\n",
    "                        unknown_words.append(word)\n",
    "    return embedding_matrix, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Title: Simple LSTM\n",
    "Author: thousandvoices\n",
    "Date: 2019\n",
    "Code version: 8\n",
    "Availability: https://www.kaggle.com/thousandvoices/simple-lstm?scriptVersionId=16109977 \"\"\"\n",
    "def build_model(embedding_matrix):\n",
    "    words = Input(shape=(MAX_LEN,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(DROPOUT_RATE)(x)\n",
    "    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(1, activation='sigmoid')(hidden)\n",
    "    model = Model(inputs=words, outputs=[result])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam') #reset learning rate using Adam Optimizer \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.395973205566406\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('comments_preprocessed/comments_preprocessed/comments_preprocessed_1.csv', index_col = 'id')\n",
    "print(df.memory_usage().sum() / 1024**2) # MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1764927, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 242 # outter_fence\n",
    "comments = df[TEXT_COLUMN].astype('str')\n",
    "y = df[TARGET_COLUMN].astype(np.int8)\n",
    "\n",
    "non_long_len = comments.apply(lambda x:len(x.split()) <= 242)\n",
    "text_removelong = comments[non_long_len].copy()\n",
    "non_long_len_indices = text_removelong.index\n",
    "y_removelong = y.loc[non_long_len_indices].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = round(y_removelong.shape[0]*TRAIN_PERCENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = text_removelong[:train_size,]\n",
    "y_train = y_removelong[:train_size,]\n",
    "x_test = text_removelong[train_size:,]\n",
    "y_test = y_removelong[train_size:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(lower = False)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242697"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_FEATURES=len(tokenizer.word_index)\n",
    "MAX_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n unknown words(crawl):  97776\n",
      "n unknown words(glove):  98663\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Title: Simple LSTM\n",
    "Author: thousandvoices\n",
    "Date: 2019\n",
    "Code version: 8\n",
    "Availability: https://www.kaggle.com/thousandvoices/simple-lstm?scriptVersionId=16109977 \"\"\"\n",
    "embedding_matrix_cl, unknown_words_cl = build_matrix(tokenizer.word_index, EMBEDDING_FILES[0])\n",
    "print('n unknown words(crawl): ', len(unknown_words_cl))\n",
    "embedding_matrix_gl, unknown_words_gl = build_matrix(tokenizer.word_index, EMBEDDING_FILES[1])\n",
    "print('n unknown words(glove): ', len(unknown_words_gl))\n",
    "\n",
    "embedding_matrix = np.concatenate([embedding_matrix_cl, embedding_matrix_gl], axis=-1)\n",
    "\n",
    "del embedding_matrix_cl\n",
    "del embedding_matrix_gl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model built\n",
      "es is set\n",
      "mc is set\n",
      "Epoch 1/20\n",
      "2069/2069 [==============================] - ETA: 0s - loss: 0.1445\n",
      "Epoch 00001: val_loss improved from inf to 0.13267, saving model to best_model.h5\n",
      "2069/2069 [==============================] - 12339s 6s/step - loss: 0.1445 - val_loss: 0.1327\n",
      "Epoch 2/20\n",
      "2069/2069 [==============================] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00002: val_loss improved from 0.13267 to 0.12970, saving model to best_model.h5\n",
      "2069/2069 [==============================] - 10957s 5s/step - loss: 0.1262 - val_loss: 0.1297\n",
      "Epoch 3/20\n",
      "2069/2069 [==============================] - ETA: 0s - loss: 0.1206\n",
      "Epoch 00003: val_loss improved from 0.12970 to 0.12434, saving model to best_model.h5\n",
      "2069/2069 [==============================] - 10934s 5s/step - loss: 0.1206 - val_loss: 0.1243\n",
      "Epoch 4/20\n",
      "2069/2069 [==============================] - ETA: 0s - loss: 0.1170\n",
      "Epoch 00004: val_loss did not improve from 0.12434\n",
      "2069/2069 [==============================] - 12144s 6s/step - loss: 0.1170 - val_loss: 0.1247\n",
      "Epoch 5/20\n",
      "2069/2069 [==============================] - ETA: 0s - loss: 0.1143\n",
      "Epoch 00005: val_loss did not improve from 0.12434\n",
      "2069/2069 [==============================] - 19530s 9s/step - loss: 0.1143 - val_loss: 0.1244\n",
      "Epoch 00005: early stopping\n",
      "model fitted\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Title: Simple LSTM\n",
    "Author: thousandvoices\n",
    "Date: 2019\n",
    "Code version: 8\n",
    "Availability: https://www.kaggle.com/thousandvoices/simple-lstm?scriptVersionId=16109977 \"\"\"\n",
    "\n",
    "# dropoutrate 0.5\n",
    "\n",
    "model = build_model(embedding_matrix)\n",
    "print('model built')\n",
    "# simple early stopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
    "print('es is set')\n",
    "mc = ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "print('mc is set')\n",
    "hist = model.fit(\n",
    "    x_train,\n",
    "    y_train_new,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    validation_split = VALID_PERCENT,\n",
    "    callbacks=[es, mc]\n",
    ")\n",
    "print('model fitted')\n",
    "predictions = model.predict(x_test, batch_size=512).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_encoded = pd.Series(predictions).apply(lambda x: 0 if x < 0.5 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = pd.DataFrame({'lstm_predict':predictions_encoded, 'id': list(y_test.index)})\n",
    "y_result = y_test_pred.merge(y_test, left_on='id', right_on=y_test.index).set_index('id')\n",
    "y_result.to_csv('result_lstm_batch512_epoch20es_imbal_nomaxf.csv')\n",
    "#y_result.to_csv('result_lstm_batch5000_epoch20es_imbal_nomaxf.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
