{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert_based_uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import transformers\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('comments_preprocessed_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.comment_text\n",
    "y_train = train.target\n",
    "df = pd.DataFrame()\n",
    "df['review'] = X_train\n",
    "df['label'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.iloc[:1058949,:]\n",
    "df_val = df.iloc[1058949:1411933,:]\n",
    "df_test = df.iloc[1411933:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Title: Deep Learning For NLP: Zero To Transformers & BERTNegative\n",
    "Author: Tanul Singh\n",
    "Date: 2020\n",
    "Availability: https://www.kaggle.com/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert\n",
    "'''\n",
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=242):\n",
    "    \"\"\"\n",
    "    Encoder for encoding the text into sequence of integers for BERT Input\n",
    "    \"\"\"\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding()\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "    \n",
    "    return np.array(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Title: Deep Learning For NLP: Zero To Transformers & BERTNegative\n",
    "Author: Tanul Singh\n",
    "Date: 2020\n",
    "Availability: https://www.kaggle.com/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert\n",
    "'''\n",
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=28996, model=BertWordPiece, add_special_tokens=True, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], clean_text=True, handle_chinese_chars=True, strip_accents=True, lowercase=False, wordpieces_prefix=##)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Title: Deep Learning For NLP: Zero To Transformers & BERTNegative\n",
    "Author: Tanul Singh\n",
    "Date: 2020\n",
    "Availability: https://www.kaggle.com/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert\n",
    "'''\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
    "# Save the loaded tokenizer locally\n",
    "tokenizer.save_pretrained('.')\n",
    "# Reload it with the huggingface tokenizers library\n",
    "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4137/4137 [01:15<00:00, 55.15it/s]\n",
      "100%|██████████| 1379/1379 [00:23<00:00, 57.98it/s]\n",
      "100%|██████████| 1379/1379 [00:23<00:00, 59.02it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Title: Deep Learning For NLP: Zero To Transformers & BERTNegative\n",
    "Author: Tanul Singh\n",
    "Date: 2020\n",
    "Availability: https://www.kaggle.com/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert\n",
    "'''\n",
    "from tqdm import tqdm\n",
    "x_train = fast_encode(df_train.review.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "x_valid = fast_encode(df_val.review.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "x_test = fast_encode(df_test.review.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = df_train.label\n",
    "y_valid = df_val.label\n",
    "y_test = df_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Title: Deep Learning For NLP: Zero To Transformers & BERTNegative\n",
    "Author: Tanul Singh\n",
    "Date: 2020\n",
    "Availability: https://www.kaggle.com/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert\n",
    "'''\n",
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_train, y_train))\n",
    "    .repeat()\n",
    "    .shuffle(2048)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_valid, y_valid))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_test, y_test))\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Title: Deep Learning For NLP: Zero To Transformers & BERTNegative\n",
    "Author: Tanul Singh\n",
    "Date: 2020\n",
    "Availability: https://www.kaggle.com/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert\n",
    "'''\n",
    "def build_model(transformer, max_len=242):\n",
    "    \"\"\"\n",
    "    function for training the BERT model\n",
    "    \"\"\"\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(cls_token)\n",
    "    \n",
    "    model = Model(inputs=input_word_ids, outputs=out)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.29 s, sys: 1.09 s, total: 8.39 s\n",
      "Wall time: 7.54 s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Title: Deep Learning For NLP: Zero To Transformers & BERTNegative\n",
    "Author: Tanul Singh\n",
    "Date: 2020\n",
    "Availability: https://www.kaggle.com/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert\n",
    "'''\n",
    "%%time\n",
    "with strategy.scope():\n",
    "    transformer_layer = (\n",
    "        transformers.TFDistilBertModel\n",
    "        .from_pretrained('distilbert-base-multilingual-cased')\n",
    "    )\n",
    "    model = build_model(transformer_layer, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 11030 steps\n",
      "11030/11030 [==============================] - 116288s 11s/step - loss: 0.1850 - accuracy: 0.9375\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Title: Deep Learning For NLP: Zero To Transformers & BERTNegative\n",
    "Author: Tanul Singh\n",
    "Date: 2020\n",
    "Availability: https://www.kaggle.com/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert\n",
    "'''\n",
    "n_steps = x_train.shape[0] // BATCH_SIZE\n",
    "train_history_2 = model.fit(\n",
    "    train_dataset.repeat(),\n",
    "    steps_per_epoch=n_steps,\n",
    "    epochs=1, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2389/11031 [=====>........................] - ETA: 7:18:38"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Title: Deep Learning For NLP: Zero To Transformers & BERTNegative\n",
    "Author: Tanul Singh\n",
    "Date: 2020\n",
    "Availability: https://www.kaggle.com/tanulsingh077/deep-learning-for-nlp-zero-to-transformers-bert\n",
    "'''\n",
    "# it has been ran on Microsfot Azure, the the output was able to convert to csv, however,\n",
    "# there was the notebook had a saving issue\n",
    "df_test['y_pred'] = model.predict(test_dataset, verbose=1)\n",
    "df_test.to_csv('actual_prediction64v2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
