{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "#pip install nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus):\n",
    "    '''This pre-processing function lemmatizes words, makes a commment case-insensitive, removes punctuation,\n",
    "    removes stop words and removes single characters. '''\n",
    "    \n",
    "    # convert df to list; *corpus* is of a form of a list of str\n",
    "    original_indices = corpus.index\n",
    "    corpus = corpus.to_list()\n",
    "    \n",
    "    # initialize lemmatizer and a list of stopwords\n",
    "    lemmatizer = spacy.load('en_core_web_sm', disable = ['parser', 'ner'])\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        text = corpus[i] # one comment\n",
    "        \n",
    "        # convert text to lower case\n",
    "        text = text.lower()\n",
    "        \n",
    "        # remove punctuations and numbers but save the \"'\"\n",
    "        symbols = \"—…,!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~0123456789\"\n",
    "        for sym in symbols:\n",
    "            text = np.char.replace(text, sym, ' ') # replace symbols by one space\n",
    "\n",
    "        # tokenize text\n",
    "        text_tokens = word_tokenize(str(text))\n",
    "\n",
    "        # spaCy Lemmatization\n",
    "        doc = lemmatizer(\" \".join([tk for tk in text_tokens]))\n",
    "        #print(doc)\n",
    "        corpus[i] = \" \".join([token.lemma_ for token in doc]) # Extract the lemma for each token and join\n",
    "    return pd.Series(corpus, index=original_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_rmsw(corpus):\n",
    "    '''remove stopword'''\n",
    "    '''This pre-processing function lemmatizes words, makes a commment case-insensitive, removes punctuation,\n",
    "    removes stop words and removes single characters. '''\n",
    "    \n",
    "    # convert df to list; *corpus* is of a form of a list of str\n",
    "    corpus = corpus.to_list()\n",
    "    \n",
    "    # initialize lemmatizer and a list of stopwords\n",
    "    lemmatizer = spacy.load('en_core_web_sm', disable = ['parser', 'ner'])\n",
    "    all_stopwords = lemmatizer.Defaults.stop_words\n",
    "    print(all_stopwords)\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        text = corpus[i] # one comment\n",
    "        \n",
    "        # convert text to lower case\n",
    "        text = text.lower()\n",
    "        \n",
    "        # remove punctuations and numbers but save the \"'\"\n",
    "        symbols = \"—…,!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~0123456789\"\n",
    "        for sym in symbols:\n",
    "            text = np.char.replace(text, sym, ' ') # replace symbols by one space\n",
    "\n",
    "        # tokenize text\n",
    "        text_tokens = word_tokenize(str(text))\n",
    "        \n",
    "        # remove stop words (\"sw\") and single characters\n",
    "        tokens_without_sw = \" \".join([word for word in text_tokens if (word not in all_stopwords) and (len(word) > 1)])\n",
    "\n",
    "        # remove \"’\"\n",
    "        tokens_without_sw = np.char.replace(tokens_without_sw, \"'\", ' ')\n",
    "        \n",
    "        # spaCy Lemmatization\n",
    "        doc = lemmatizer(str(tokens_without_sw))\n",
    "        \n",
    "        corpus[i] = \" \".join([token.lemma_ for token in doc]) # Extract the lemma for each token and join\n",
    "    return corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
