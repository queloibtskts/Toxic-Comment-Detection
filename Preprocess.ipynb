{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "#pip install nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "text = u'This is a smiley face \\U0001f602'\n",
    "print(text) # with emoji\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "print(deEmojify(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a smiley face ðŸ˜‚\n",
      "This is a smiley face \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = u'This is a smiley face \\U0001f602'\n",
    "print(text) # with emoji\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "print(deEmojify(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus):\n",
    "    '''This pre-processing function lemmatizes words, makes a commment case-insensitive, removes punctuation,\n",
    "    removes stop words and removes single characters. '''\n",
    "    \n",
    "    # convert df to list; *corpus* is of a form of a list of str\n",
    "    original_indices = corpus.index\n",
    "    corpus = corpus.to_list()\n",
    "    \n",
    "    # initialize lemmatizer and a list of stopwords\n",
    "    lemmatizer = spacy.load('en_core_web_sm', disable = ['parser', 'ner'])\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        text = corpus[i] # one comment\n",
    "        \n",
    "        # convert text to lower case\n",
    "        text = text.lower()\n",
    "        \n",
    "        # remove emoji\n",
    "        text = deEmojify(text)\n",
    "        \n",
    "        # remove punctuations and numbers but save the \"'\"\n",
    "        symbols = \"â€”â€¦,!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~0123456789\"\n",
    "        for sym in symbols:\n",
    "            text = np.char.replace(text, sym, ' ') # replace symbols by one space\n",
    "\n",
    "        # tokenize text\n",
    "        text_tokens = word_tokenize(str(text))\n",
    "\n",
    "        # spaCy Lemmatization\n",
    "        doc = lemmatizer(\" \".join([tk for tk in text_tokens]))\n",
    "        #print(doc)\n",
    "        corpus[i] = \" \".join([token.lemma_ for token in doc]) # Extract the lemma for each token and join\n",
    "    return pd.Series(corpus, index=original_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_rmsw(corpus):\n",
    "    '''remove stopword'''\n",
    "    '''This pre-processing function lemmatizes words, makes a commment case-insensitive, removes punctuation,\n",
    "    removes stop words and removes single characters. '''\n",
    "    \n",
    "    # convert df to list; *corpus* is of a form of a list of str\n",
    "    corpus = corpus.to_list()\n",
    "    \n",
    "    # initialize lemmatizer and a list of stopwords\n",
    "    lemmatizer = spacy.load('en_core_web_sm', disable = ['parser', 'ner'])\n",
    "    all_stopwords = lemmatizer.Defaults.stop_words\n",
    "    print(all_stopwords)\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        text = corpus[i] # one comment\n",
    "        \n",
    "        # convert text to lower case\n",
    "        text = text.lower()\n",
    "        \n",
    "        # remove punctuations and numbers but save the \"'\"\n",
    "        symbols = \"â€”â€¦,!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~0123456789\"\n",
    "        for sym in symbols:\n",
    "            text = np.char.replace(text, sym, ' ') # replace symbols by one space\n",
    "\n",
    "        # tokenize text\n",
    "        text_tokens = word_tokenize(str(text))\n",
    "        \n",
    "        # remove stop words (\"sw\") and single characters\n",
    "        tokens_without_sw = \" \".join([word for word in text_tokens if (word not in all_stopwords) and (len(word) > 1)])\n",
    "\n",
    "        # remove \"â€™\"\n",
    "        tokens_without_sw = np.char.replace(tokens_without_sw, \"'\", ' ')\n",
    "        \n",
    "        # spaCy Lemmatization\n",
    "        doc = lemmatizer(str(tokens_without_sw))\n",
    "        \n",
    "        corpus[i] = \" \".join([token.lemma_ for token in doc]) # Extract the lemma for each token and join\n",
    "    return corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
