{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 120\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import spacy\n",
    "#pip install nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "#import en_core_web_sm\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, cross_val_predict\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv(\"data/comments_preprocessed_final.csv\")\n",
    "comments_50k = comments.iloc[:50000,]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_50k = comments_50k.comment_text\n",
    "y_50k = comments_50k.target\n",
    "X = comments.comment_text\n",
    "y = comments.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = 0.8\n",
    "train_size = int(X.shape[0] * percent)\n",
    "X_train = X.iloc[:train_size,]\n",
    "X_valid = X.iloc[train_size:,]\n",
    "y_train = y.iloc[:train_size,]\n",
    "y_valid = y.iloc[train_size:,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use count vectorizer\n",
    "# partly retreived from https://www.kaggle.com/catris25/multinomial-naive-bayes-with-countvectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_cv = vectorizer.fit_transform(X_train)\n",
    "X_valid_cv = vectorizer.transform(X_valid)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_cv, y_train)\n",
    "y_pred_nb = mnb.predict(X_valid_cv)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_valid, y_pred_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output nb with cv csv\n",
    "y_pred_nb_df = pd.DataFrame({'pred':y_pred_nb, 'id': list(y_valid.index)})\n",
    "y_result = y_pred_nb_df.merge(y_valid, left_on='id', right_on=y_valid.index).set_index('id')\n",
    "y_result.to_csv('result_nb_8-2.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# analysis on comments identity\n",
    "\n",
    "unprocessed = pd.read_csv(\"data/train.csv\")\n",
    "cols = ['id', 'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish', 'muslim', 'black', 'white']\n",
    "#cols = unprocessed.columns[0:32]\n",
    "identity = unprocessed[cols].copy()\n",
    "#identity = identity.drop(['target', 'comment_text', 'severe_toxicity', 'obscene', 'identity_attack'], axis=1)\n",
    "identity.head(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "valid_full = pd.concat([identity,y_valid], axis=1, join='inner').reset_index().iloc[:,1:]\n",
    "valid_full = valid_full.rename(columns = {'target': 'fact'}, inplace = False)\n",
    "valid_full['pred'] = pd.Series(y_pred_nb)\n",
    "valid_full = valid_full.fillna(0)\n",
    "valid_full.head(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fp_fn = valid_full[valid_full['fact'] != valid_full['pred']]\n",
    "fp = fp_fn[fp_fn['pred'] == 1]\n",
    "fn = fp_fn[fp_fn['pred'] == 0]\n",
    "print('There are %d False Positive (predit to be toxic, however not) ' %fp.shape[0])\n",
    "print('There are %d False Negative (predit not to be toxic, however yes) ' %fn.shape[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# analyse FP\n",
    "cols_name = cols[1:]\n",
    "fp_dict = defaultdict(int) \n",
    "for row in range(fp.shape[0]):\n",
    "    for column in cols_name:\n",
    "        if fp.iloc[row][column] != 0:\n",
    "            fp_dict[column] += 1\n",
    "print('The count of false positve belonging to these identities:')\n",
    "key_pairs = [ (v,k) for k,v in fp_dict.items() ]\n",
    "key_pairs.sort(reverse=True)\n",
    "for val, key in key_pairs:\n",
    "    print (\"%s: %d\" % (key, val))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# analyse FN\n",
    "fn_dict = defaultdict(int) \n",
    "for row in range(fn.shape[0]):\n",
    "    for column in cols_name:\n",
    "        if fn.iloc[row][column] != 0:\n",
    "            fn_dict[column] += 1\n",
    "print('The count of false negative belonging to these identities:')\n",
    "key_pairs = [ (v,k) for k,v in fn_dict.items() ]\n",
    "key_pairs.sort(reverse=True)\n",
    "for val, key in key_pairs:\n",
    "    print (\"%s: %d\" % (key, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------------------')\n",
    "print('Result for Multinomial Naive Bayes')\n",
    "accuracy = round(accuracy_score(y_valid, y_pred_nb), 4)\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "precision = round(precision_score(y_valid, y_pred_nb), 4)\n",
    "print('Precision: %.4f' % precision)\n",
    "recall = round(recall_score(y_valid, y_pred_nb), 4)\n",
    "print('Recall: %.4f' % recall)\n",
    "score = round(f1_score(y_valid, y_pred_nb), 4)\n",
    "print('F1_score: %.4f' % score)\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# use tf-idf\n",
    "# retreived from https://iq.opengenus.org/naive-bayes-on-tf-idf-vectorized-matrix/\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tf = tf_vectorizer.fit_transform(X_train)\n",
    "X_valid_tf = tf_vectorizer.transform(X_valid)\n",
    "print(\"X_train has n_samples: %d, n_features: %d\" % X_train_tf.shape)\n",
    "print(\"X_valid has n_samples: %d, n_features: %d\" % X_valid_tf.shape)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_tf, y_train)\n",
    "y_pred = mnb.predict(X_valid_tf)\n",
    "\n",
    "score = metrics.accuracy_score(y_valid, y_pred)\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "print(metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_valid, y_pred))\n",
    "\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Cross Validation for count vectorizer\n",
    "X_cv = vectorizer.fit_transform(X)\n",
    "scores_5fd = cross_val_score(mnb, X_cv, y, cv=5)\n",
    "print(\"Cross validated scores:\", scores_5fd)\n",
    "print(\"Mean cross validated scores:\", scores_5fd.mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Cross Validation for tfidf\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer() \n",
    "\n",
    "X_tf = tf_vectorizer.fit_transform(X)\n",
    "#X_valid_tf = tf_vectorizer.transform(X_valid)\n",
    "print(\"X_train has n_samples: %d, n_features: %d\" % X_tf.shape)\n",
    "#print(\"X_valid has n_samples: %d, n_features: %d\" % X_valid_tf.shape)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "scores_5fd = cross_val_score(mnb, X_tf, y, cv=5)\n",
    "print(\"Cross validated scores:\", scores_5fd)\n",
    "print(\"Mean cross validated scores:\", scores_5fd.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plain SVM"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# small-sized testing\n",
    "size = 50000\n",
    "X_smsize = X.iloc[:size,]\n",
    "y_smsize = y.iloc[:size,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = 0.8\n",
    "train_size = int(X.shape[0] * percent)\n",
    "X_train = X.iloc[:train_size,]\n",
    "X_valid = X.iloc[train_size:,]\n",
    "y_train = y.iloc[:train_size,]\n",
    "y_valid = y.iloc[train_size:,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "pipeline = Pipeline([\n",
    "                    ('tfidf', TfidfVectorizer()), \n",
    "                    ('SVC', svm.SVC()), \n",
    "                    ])\n",
    "parameters = {\n",
    "    'tfidf__stop_words': ['english'],\n",
    "    'tfidf__max_features': [1000, 5000, 10000, 50000],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'SVC__C': [1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "    'SVC__kernel': ['linear'],\n",
    "    'SVC__gamma': ['auto'],\n",
    "    'SVC__max_iter': [5e2, 1e3, 5e3]\n",
    "}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    grid_search=GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1, verbose=10)\n",
    "    \n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    print(\"done in %0.3fmin\" % ((time() - t0)/60))\n",
    "    print()\n",
    "    \n",
    "    results = grid_search.cv_results_\n",
    "    test_scores = results['mean_test_score']\n",
    "    params = results['params']\n",
    "    rank = results['rank_test_score']\n",
    "    \n",
    "    print(\"best score: %0.4f\" % grid_search.best_score_)\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    print(\"Best parameters set: \")\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "    #print(\"Top 5 best parameter sets based on accuracy score: \")\n",
    "    #list(np.take(params,list(rank.argsort()[:5])))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use optimal parameters for SVM\n",
    "tf_vectorizer_svm = TfidfVectorizer(max_features=50000, ngram_range = (1, 2), stop_words = 'english')\n",
    "tf_vectorizer_svm.fit(X_train)\n",
    "X_train_svm = tf_vectorizer_svm.transform(X_train)\n",
    "X_valid_svm = tf_vectorizer_svm.transform(X_valid)\n",
    "\n",
    "SVM = svm.SVC(C=10, kernel='linear', gamma='auto', max_iter = 1000)\n",
    "SVM.fit(X_train_svm,y_train)\n",
    "y_pred_svm = SVM.predict(X_valid_svm)\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(y_valid, y_pred_svm)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------------------')\n",
    "print('Result for Support Vector Machine')\n",
    "accuracy = accuracy_score(y_valid, y_pred_svm)\n",
    "print('Accuracy: %.4f' % accuracy)\n",
    "precision = precision_score(y_valid, y_pred_svm)\n",
    "print('Precision: %.4f' % precision)\n",
    "recall = recall_score(y_valid, y_pred_svm)\n",
    "print('Recall: %.4f' % recall)\n",
    "score = f1_score(y_valid, y_pred_svm)\n",
    "print('F1_score: %.4f' % score)\n",
    "print('------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output csv\n",
    "y_pred_svm_df = pd.DataFrame({'pred':y_pred_svm, 'id': list(y_valid.index)})\n",
    "y_result = y_pred_svm_df.merge(y_valid, left_on='id', right_on=y_valid.index).set_index('id')\n",
    "y_result.to_csv('result_svm_8-2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
